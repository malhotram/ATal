from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from pydantic import BaseModel
from typing import List
import tempfile
import os
from dotenv import load_dotenv
load_dotenv()
app = FastAPI(title="⛑️ Mine Survival Assistant API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

os.environ["HF_TOKEN"]=os.getenv("HF_TOKEN")
os.environ["GOOGLE_API_KEY"]=os.getenv("GEMINI_KEY")

embedding_model = HuggingFaceEmbeddings(model="all-MiniLM-L6-v2")
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

vectorstores = {}
chat_histories = {}

class AskRequest(BaseModel):
    session_id: str
    question: str

@app.get("/health")
def health():
    return {"status": "ok"}

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/upload")
async def upload_pdfs(
    session_id: str = Form(...),
    files: List[UploadFile] = File(...)
):
    if not files:
        return JSONResponse({"error": "No PDF files uploaded"}, status_code=400)

    documents = []
    for uf in files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(await uf.read())
            tmp_path = tmp.name
        loader = PyPDFLoader(tmp_path)
        documents.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    splits = splitter.split_documents(documents)
    vs = Chroma.from_documents(documents=splits, embedding=embedding_model)
    retriever = vs.as_retriever()

    vectorstores[session_id] = retriever
    chat_histories[session_id] = ChatMessageHistory()

    return {"message": f"{len(files)} PDFs uploaded for session '{session_id}'"}

@app.post("/ask")
async def ask_question(req: AskRequest):
    session_id = req.session_id
    question = req.question

    if session_id not in vectorstores:
        return JSONResponse(
            {"error": "No documents uploaded for this session. POST /upload first."},
            status_code=400,
        )

    retriever = vectorstores[session_id]

    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question, "
        "formulate a standalone question which can be understood without the chat history. "
        "Do NOT answer the question, just reformulate it if needed."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [("system", contextualize_q_system_prompt),
         MessagesPlaceholder("chat_history"),
         ("human", "{input}")]
    )
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
